from typing import List
from relativisticpy.parsers.lexers.base import BaseLexer
from relativisticpy.parsers.parsers.base import BaseParser


class TreeBuilder:

    def __init__(self, 
                    lexer:                  BaseLexer, 
                    parser:                 BaseParser, 
                    semantic_analyzer:      BaseSemanticAnalyzer,
                ):
        self.lexer                  = lexer
        self.parser                 = parser
        self.semantic_analyzer      = semantic_analyzer

    def build(self) -> IParser:
        # Instantiate provider:
        node_provider_instance  = self._build_node_provider()


        # Configure node provider to the custom, user defined, variables and functions methods.
        node_provider_instance.set_matcher(self.configuration_models if self.configuration_models != None else [])

        # Build the Parser object from tokens generated by user input:
        parser_instance = self.parser(node_provider_instance)

        # Return an instance of Parser object, which implements IParser:
        return parser_instance

    def parse_string_service(self, string: str):
        # Generate Tokens from stirng
        tokens = self.tokenize_string_service(string)

        # Wrap the tokens around the iterable once more:
        tokens_iter_instance = self._build_iterator(tokens)

        semantic_analyzer_instance = self._build_semantic_analyzer()

        # Build parser instance
        parser_instace = self.build()

        AST = parser_instace.parse(tokens_iter_instance)

        AST = semantic_analyzer_instance.analyse_tree(AST)

        return AST
    
    def analyse_ast(self, ast):
        return self.semantic_analyzer.analyse_tree(ast)

    def tokenize_string_service(self, string: str) -> List[Token]:
        # Instantiate providers:
        token_provider_instance = self._build_token_provider()

        # Wrap the text we wish to parse around an iterator object:
        character_iter_instance = self._build_iterator(string)

        # Build Lexer with user input:
        lexer = self._build_lexer(token_provider_instance)

        # Tokenize the input:
        tokens = lexer.tokenize(character_iter_instance)

        return tokens

    def _build_node_provider(self) -> INodeProvider:
        return self.node_provider()

    def _build_token_provider(self) -> ITokenProvider:
        return self.token_provider()

    def _build_lexer(self, token_provider: ITokenProvider) -> ILexer:
        return self.lexer(token_provider)

    def _build_parser(self, token_provider: ITokenProvider) -> ILexer:
        return self.parser(token_provider)
    
    def _build_iterator(self, type) -> IIterator:
        return self.iterator(type)

    def _build_semantic_analyzer(self) -> ISemanticAnalyzer:
        return self.semantic_analyzer()