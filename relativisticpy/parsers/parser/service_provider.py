from typing import List
from relativisticpy.parsers.shared.interfaces.iterator import IIterator
from relativisticpy.parsers.shared.interfaces.lexer import ILexer
from relativisticpy.parsers.shared.interfaces.node_provider import INodeProvider
from relativisticpy.parsers.shared.interfaces.parser_ import IParser
from relativisticpy.parsers.shared.interfaces.tokens import ITokenProvider
from relativisticpy.parsers.shared.models.node_keys import ConfigurationModels
from relativisticpy.parsers.shared.models.token import Token

class ParserServicesProvider:

    def __init__(self, 
                    lexer:                  ILexer, 
                    parser:                 IParser, 
                    token_provider:         ITokenProvider, 
                    node_provider:          INodeProvider, 
                    iterator:               IIterator, 
                    configuration_models:   ConfigurationModels
                ):
        self.token_provider         = token_provider
        self.node_provider          = node_provider
        self.configuration_models   = configuration_models
        self.lexer                  = lexer
        self.parser                 = parser
        self.iterator               = iterator

    def build(self) -> IParser:
        # Instantiate provider:
        node_provider_instance  = self._build_node_provider()

        # Configure node provider to the custom, user defined, variables and functions methods.
        node_provider_instance.set_matcher(self.configuration_models if self.configuration_models != None else [])

        # Build the Parser object from tokens generated by user input:
        parser_instance = self.parser(node_provider_instance)

        # Return an instance of Parser object, which implements IParser:
        return parser_instance

    def parse_string_service(self, string: str):
        # Generate Tokens from stirng
        tokens = self.tokenize_string_service(string)

        # Wrap the tokens around the iterable once more:
        tokens_iter_instance = self._build_iterator(tokens)

        # Build parser instance
        parser_instace = self.build()

        AST = parser_instace.parse(tokens_iter_instance)

        return AST

    def tokenize_string_service(self, string: str) -> List[Token]:
        # Instantiate providers:
        token_provider_instance = self._build_token_provider()

        # Wrap the text we wish to parse around an iterator object:
        character_iter_instance = self._build_iterator(string)

        # Build Lexer with user input:
        lexer = self._build_lexer(token_provider_instance)

        # Tokenize the input:
        tokens = lexer.tokenize(character_iter_instance)

        return tokens

    def _build_node_provider(self) -> INodeProvider:
        return self.node_provider()

    def _build_token_provider(self) -> ITokenProvider:
        return self.token_provider()

    def _build_lexer(self, token_provider: ITokenProvider) -> ILexer:
        return self.lexer(token_provider)

    def _build_parser(self, token_provider: ITokenProvider) -> ILexer:
        return self.parser(token_provider)
    
    def _build_iterator(self, type) -> IIterator:
        return self.iterator(type)
